{
  "dataset_revision": "4672e20407010da34463acc759c162ca9734bca6",
  "evaluation_time": 3537.4775993824005,
  "kg_co2_emissions": null,
  "mteb_version": "1.12.85",
  "scores": {
    "test": [
      {
        "accuracy": 0.6325151311365165,
        "f1": 0.5975882977426797,
        "f1_weighted": 0.6392032473873075,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.6325151311365165,
        "scores_per_experiment": [
          {
            "accuracy": 0.6331540013449899,
            "f1": 0.6072246534501038,
            "f1_weighted": 0.6419392223430952
          },
          {
            "accuracy": 0.6472763954270343,
            "f1": 0.607001996820436,
            "f1_weighted": 0.6525472525046037
          },
          {
            "accuracy": 0.6449226630800269,
            "f1": 0.5984627819824767,
            "f1_weighted": 0.64086177279054
          },
          {
            "accuracy": 0.6455951580363147,
            "f1": 0.6086320864264336,
            "f1_weighted": 0.6561045489362786
          },
          {
            "accuracy": 0.6318090114324143,
            "f1": 0.5946698623469319,
            "f1_weighted": 0.6406928877856756
          },
          {
            "accuracy": 0.6153328850033625,
            "f1": 0.5857184329016031,
            "f1_weighted": 0.6256492617000736
          },
          {
            "accuracy": 0.628782784129119,
            "f1": 0.6083285975600183,
            "f1_weighted": 0.6324866430806433
          },
          {
            "accuracy": 0.6294552790854069,
            "f1": 0.5840247477396098,
            "f1_weighted": 0.6436430787984683
          },
          {
            "accuracy": 0.6075991930060525,
            "f1": 0.5760451264373218,
            "f1_weighted": 0.6149404425626583
          },
          {
            "accuracy": 0.6412239408204439,
            "f1": 0.605774691761862,
            "f1_weighted": 0.6431673633710375
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.6380718150516478,
        "f1": 0.6058540989147829,
        "f1_weighted": 0.6429002183289543,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.6380718150516478,
        "scores_per_experiment": [
          {
            "accuracy": 0.6355140186915887,
            "f1": 0.6219075298994347,
            "f1_weighted": 0.6408501987057498
          },
          {
            "accuracy": 0.6487948844072798,
            "f1": 0.6160211212314616,
            "f1_weighted": 0.6522821052080369
          },
          {
            "accuracy": 0.6620757501229709,
            "f1": 0.6161366938100381,
            "f1_weighted": 0.6608259983626634
          },
          {
            "accuracy": 0.6335464830300049,
            "f1": 0.5955293873480731,
            "f1_weighted": 0.6394144477483025
          },
          {
            "accuracy": 0.6399409739301525,
            "f1": 0.6183273855716245,
            "f1_weighted": 0.6452298987174203
          },
          {
            "accuracy": 0.6242006886374816,
            "f1": 0.5942135087596824,
            "f1_weighted": 0.6347665847440747
          },
          {
            "accuracy": 0.6237088047220856,
            "f1": 0.5957619052630733,
            "f1_weighted": 0.6250225372613669
          },
          {
            "accuracy": 0.6374815543531727,
            "f1": 0.5950161778943048,
            "f1_weighted": 0.6452737683784364
          },
          {
            "accuracy": 0.6212493851451057,
            "f1": 0.5965800089107358,
            "f1_weighted": 0.6287856945230811
          },
          {
            "accuracy": 0.6542056074766355,
            "f1": 0.6090472704594005,
            "f1_weighted": 0.6565509496404112
          }
        ]
      }
    ]
  },
  "task_name": "MassiveIntentClassification"
}