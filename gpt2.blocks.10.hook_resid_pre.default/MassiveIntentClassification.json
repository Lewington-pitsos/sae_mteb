{
  "dataset_revision": "4672e20407010da34463acc759c162ca9734bca6",
  "evaluation_time": 905.052670955658,
  "kg_co2_emissions": null,
  "mteb_version": "1.12.85",
  "scores": {
    "test": [
      {
        "accuracy": 0.503866845998655,
        "f1": 0.4971148060651605,
        "f1_weighted": 0.511740252246032,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.503866845998655,
        "scores_per_experiment": [
          {
            "accuracy": 0.5164761264290518,
            "f1": 0.5071943468627979,
            "f1_weighted": 0.520980711575515
          },
          {
            "accuracy": 0.48856758574310694,
            "f1": 0.47682493264490716,
            "f1_weighted": 0.4929559859947577
          },
          {
            "accuracy": 0.5104236718224613,
            "f1": 0.5091613266916851,
            "f1_weighted": 0.5233107362154216
          },
          {
            "accuracy": 0.4912575655682582,
            "f1": 0.492505062352909,
            "f1_weighted": 0.5110665337607702
          },
          {
            "accuracy": 0.5211835911230666,
            "f1": 0.4934275019670607,
            "f1_weighted": 0.525880486732818
          },
          {
            "accuracy": 0.49932750504371215,
            "f1": 0.4967046059618878,
            "f1_weighted": 0.5065411317261642
          },
          {
            "accuracy": 0.4969737726967048,
            "f1": 0.4936369090667693,
            "f1_weighted": 0.4946947697470067
          },
          {
            "accuracy": 0.495965030262273,
            "f1": 0.48915926890205724,
            "f1_weighted": 0.5088932760421528
          },
          {
            "accuracy": 0.47948890383322124,
            "f1": 0.48645135877670703,
            "f1_weighted": 0.4898964101050611
          },
          {
            "accuracy": 0.539004707464694,
            "f1": 0.5260827474248243,
            "f1_weighted": 0.5431824805606532
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.5156910969011314,
        "f1": 0.5051650439239712,
        "f1_weighted": 0.5208818100191758,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.5156910969011314,
        "scores_per_experiment": [
          {
            "accuracy": 0.5258239055582883,
            "f1": 0.5131068910683568,
            "f1_weighted": 0.5265825949823221
          },
          {
            "accuracy": 0.5036891293654697,
            "f1": 0.4860028118233652,
            "f1_weighted": 0.5042798193430849
          },
          {
            "accuracy": 0.5518937530742745,
            "f1": 0.5434777438425294,
            "f1_weighted": 0.5649561743740367
          },
          {
            "accuracy": 0.4972946384653222,
            "f1": 0.4937658285006723,
            "f1_weighted": 0.5076456271135505
          },
          {
            "accuracy": 0.5395966551893753,
            "f1": 0.5161524624505114,
            "f1_weighted": 0.5423915805411725
          },
          {
            "accuracy": 0.5154943433349729,
            "f1": 0.5141771751960705,
            "f1_weighted": 0.5201924226391192
          },
          {
            "accuracy": 0.4928676832267585,
            "f1": 0.489778479970099,
            "f1_weighted": 0.49225621162661143
          },
          {
            "accuracy": 0.49090014756517464,
            "f1": 0.4804550475894074,
            "f1_weighted": 0.4967819024257901
          },
          {
            "accuracy": 0.49090014756517464,
            "f1": 0.4895042373674148,
            "f1_weighted": 0.5008240847770756
          },
          {
            "accuracy": 0.5484505656665027,
            "f1": 0.5252297614312842,
            "f1_weighted": 0.5529076823689946
          }
        ]
      }
    ]
  },
  "task_name": "MassiveIntentClassification"
}