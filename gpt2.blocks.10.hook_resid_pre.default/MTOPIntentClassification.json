{
  "dataset_revision": "ae001d0e6b1228650b7bd1c2c65fb50ad11a8aba",
  "evaluation_time": 2025.7336604595184,
  "kg_co2_emissions": null,
  "mteb_version": "1.12.85",
  "scores": {
    "test": [
      {
        "accuracy": 0.5522343821249429,
        "f1": 0.388698872604491,
        "f1_weighted": 0.6050112331489539,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.5522343821249429,
        "scores_per_experiment": [
          {
            "accuracy": 0.5246238030095759,
            "f1": 0.35830638780082735,
            "f1_weighted": 0.5784176736860778
          },
          {
            "accuracy": 0.5585955312357501,
            "f1": 0.39671707649273646,
            "f1_weighted": 0.613220935705396
          },
          {
            "accuracy": 0.5693114455084359,
            "f1": 0.3889353623426236,
            "f1_weighted": 0.6190592206712019
          },
          {
            "accuracy": 0.5583675330597355,
            "f1": 0.3842901157716006,
            "f1_weighted": 0.6144333497716136
          },
          {
            "accuracy": 0.5542635658914729,
            "f1": 0.39797008457389277,
            "f1_weighted": 0.597734575876938
          },
          {
            "accuracy": 0.5191518467852257,
            "f1": 0.3911315228031527,
            "f1_weighted": 0.5797381658175438
          },
          {
            "accuracy": 0.5357957136342909,
            "f1": 0.38787698298153483,
            "f1_weighted": 0.5923024528511824
          },
          {
            "accuracy": 0.5930232558139535,
            "f1": 0.3899579588245424,
            "f1_weighted": 0.6466104968863532
          },
          {
            "accuracy": 0.5599635202918377,
            "f1": 0.39596062827448497,
            "f1_weighted": 0.6081993086897797
          },
          {
            "accuracy": 0.5492476060191519,
            "f1": 0.3958426061795144,
            "f1_weighted": 0.6003961515334523
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.5502460850111857,
        "f1": 0.36853615398872014,
        "f1_weighted": 0.6047111971690446,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.5502460850111857,
        "scores_per_experiment": [
          {
            "accuracy": 0.5002237136465324,
            "f1": 0.33836431856882415,
            "f1_weighted": 0.5567190254971968
          },
          {
            "accuracy": 0.5753914988814318,
            "f1": 0.3832963134166781,
            "f1_weighted": 0.634162033765062
          },
          {
            "accuracy": 0.5771812080536913,
            "f1": 0.38143412614694094,
            "f1_weighted": 0.6265083913050447
          },
          {
            "accuracy": 0.5364653243847874,
            "f1": 0.34993096556511055,
            "f1_weighted": 0.5958526176979029
          },
          {
            "accuracy": 0.5579418344519016,
            "f1": 0.3789010739880956,
            "f1_weighted": 0.6031697837387981
          },
          {
            "accuracy": 0.5217002237136465,
            "f1": 0.37955413718929293,
            "f1_weighted": 0.5824908507286508
          },
          {
            "accuracy": 0.5387024608501119,
            "f1": 0.36505149250075153,
            "f1_weighted": 0.5929145011609358
          },
          {
            "accuracy": 0.5910514541387024,
            "f1": 0.38549604322833714,
            "f1_weighted": 0.6453372578692087
          },
          {
            "accuracy": 0.5476510067114094,
            "f1": 0.3671551650161749,
            "f1_weighted": 0.5998245466585086
          },
          {
            "accuracy": 0.5561521252796421,
            "f1": 0.3561779042669958,
            "f1_weighted": 0.6101329632691388
          }
        ]
      }
    ]
  },
  "task_name": "MTOPIntentClassification"
}