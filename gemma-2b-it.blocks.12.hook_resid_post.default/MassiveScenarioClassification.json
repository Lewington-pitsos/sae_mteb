{
  "dataset_revision": "fad2c6e8459f9e1c45d9315f4953d921437d70f8",
  "evaluation_time": 1836.6856813430786,
  "kg_co2_emissions": null,
  "mteb_version": "1.12.85",
  "scores": {
    "test": [
      {
        "accuracy": 0.6493275050437122,
        "f1": 0.634354631811727,
        "f1_weighted": 0.652739820415763,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.6493275050437122,
        "scores_per_experiment": [
          {
            "accuracy": 0.691324815063887,
            "f1": 0.6755690000728043,
            "f1_weighted": 0.6942279217511034
          },
          {
            "accuracy": 0.6805648957632818,
            "f1": 0.6630922985122212,
            "f1_weighted": 0.6818291526282821
          },
          {
            "accuracy": 0.6200403496973773,
            "f1": 0.6152651605904986,
            "f1_weighted": 0.6272400557262955
          },
          {
            "accuracy": 0.6412239408204439,
            "f1": 0.6182747054827,
            "f1_weighted": 0.6445970791806591
          },
          {
            "accuracy": 0.6476126429051782,
            "f1": 0.626929462548921,
            "f1_weighted": 0.6554731093376095
          },
          {
            "accuracy": 0.6432414256893073,
            "f1": 0.62513869654209,
            "f1_weighted": 0.6473470786741425
          },
          {
            "accuracy": 0.6550100874243443,
            "f1": 0.6482449170791738,
            "f1_weighted": 0.6562968824513928
          },
          {
            "accuracy": 0.6472763954270343,
            "f1": 0.6299872897863594,
            "f1_weighted": 0.6528625146746461
          },
          {
            "accuracy": 0.6398789509078682,
            "f1": 0.6287316199721068,
            "f1_weighted": 0.6397798971437001
          },
          {
            "accuracy": 0.6271015467383995,
            "f1": 0.6123131675303956,
            "f1_weighted": 0.6277445125897977
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.641219872110182,
        "f1": 0.6321297839867188,
        "f1_weighted": 0.6456335908324804,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.641219872110182,
        "scores_per_experiment": [
          {
            "accuracy": 0.691096901131333,
            "f1": 0.6825280652934602,
            "f1_weighted": 0.6962556085822968
          },
          {
            "accuracy": 0.6606000983767831,
            "f1": 0.6497773463995892,
            "f1_weighted": 0.6635594552407849
          },
          {
            "accuracy": 0.6237088047220856,
            "f1": 0.6193512608037932,
            "f1_weighted": 0.6303259656165395
          },
          {
            "accuracy": 0.6094441711756026,
            "f1": 0.5943886829194738,
            "f1_weighted": 0.6150947067000143
          },
          {
            "accuracy": 0.6404328578455485,
            "f1": 0.628331600127164,
            "f1_weighted": 0.646723742419538
          },
          {
            "accuracy": 0.6251844564682735,
            "f1": 0.6191678601096947,
            "f1_weighted": 0.6274130168169886
          },
          {
            "accuracy": 0.6414166256763404,
            "f1": 0.6366080340157401,
            "f1_weighted": 0.6463830587601123
          },
          {
            "accuracy": 0.6507624200688638,
            "f1": 0.642082323199511,
            "f1_weighted": 0.6558292472254588
          },
          {
            "accuracy": 0.6379734382685687,
            "f1": 0.6314204585191123,
            "f1_weighted": 0.6403707366735634
          },
          {
            "accuracy": 0.631578947368421,
            "f1": 0.6176422084796485,
            "f1_weighted": 0.6343803702895068
          }
        ]
      }
    ]
  },
  "task_name": "MassiveScenarioClassification"
}