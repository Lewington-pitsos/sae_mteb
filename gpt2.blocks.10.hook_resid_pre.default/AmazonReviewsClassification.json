{
  "dataset_revision": "1399c76144fd37290681b995c656ef9b2e06e26d",
  "evaluation_time": 368.6026225090027,
  "kg_co2_emissions": null,
  "mteb_version": "1.12.85",
  "scores": {
    "test": [
      {
        "accuracy": 0.31222000000000005,
        "f1": 0.3106156913253195,
        "f1_weighted": 0.3106156913253195,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.31222000000000005,
        "scores_per_experiment": [
          {
            "accuracy": 0.3592,
            "f1": 0.35352082010203406,
            "f1_weighted": 0.353520820102034
          },
          {
            "accuracy": 0.3358,
            "f1": 0.32893547046482874,
            "f1_weighted": 0.3289354704648288
          },
          {
            "accuracy": 0.3078,
            "f1": 0.3040679852923474,
            "f1_weighted": 0.3040679852923474
          },
          {
            "accuracy": 0.3258,
            "f1": 0.3238836658696137,
            "f1_weighted": 0.32388366586961365
          },
          {
            "accuracy": 0.3264,
            "f1": 0.3194110646999794,
            "f1_weighted": 0.3194110646999794
          },
          {
            "accuracy": 0.2896,
            "f1": 0.2901821677513955,
            "f1_weighted": 0.2901821677513955
          },
          {
            "accuracy": 0.2642,
            "f1": 0.2683688307898568,
            "f1_weighted": 0.26836883078985685
          },
          {
            "accuracy": 0.2934,
            "f1": 0.29042152714391606,
            "f1_weighted": 0.29042152714391606
          },
          {
            "accuracy": 0.315,
            "f1": 0.31513978797808967,
            "f1_weighted": 0.31513978797808967
          },
          {
            "accuracy": 0.305,
            "f1": 0.31222559316113374,
            "f1_weighted": 0.31222559316113374
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.31268,
        "f1": 0.31103180458311164,
        "f1_weighted": 0.31103180458311164,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.31268,
        "scores_per_experiment": [
          {
            "accuracy": 0.3472,
            "f1": 0.34222071469763005,
            "f1_weighted": 0.3422207146976301
          },
          {
            "accuracy": 0.3378,
            "f1": 0.3299817044806216,
            "f1_weighted": 0.32998170448062164
          },
          {
            "accuracy": 0.3234,
            "f1": 0.3198955263106646,
            "f1_weighted": 0.3198955263106646
          },
          {
            "accuracy": 0.3322,
            "f1": 0.3324465032447991,
            "f1_weighted": 0.3324465032447991
          },
          {
            "accuracy": 0.324,
            "f1": 0.3152425613156613,
            "f1_weighted": 0.31524256131566125
          },
          {
            "accuracy": 0.2946,
            "f1": 0.2937875271701519,
            "f1_weighted": 0.2937875271701519
          },
          {
            "accuracy": 0.2696,
            "f1": 0.272239361369271,
            "f1_weighted": 0.27223936136927107
          },
          {
            "accuracy": 0.2844,
            "f1": 0.2815455800367934,
            "f1_weighted": 0.2815455800367934
          },
          {
            "accuracy": 0.314,
            "f1": 0.31512517215953,
            "f1_weighted": 0.31512517215953
          },
          {
            "accuracy": 0.2996,
            "f1": 0.30783339504599355,
            "f1_weighted": 0.30783339504599355
          }
        ]
      }
    ]
  },
  "task_name": "AmazonReviewsClassification"
}