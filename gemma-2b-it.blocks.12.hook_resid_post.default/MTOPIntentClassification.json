{
  "dataset_revision": "ae001d0e6b1228650b7bd1c2c65fb50ad11a8aba",
  "evaluation_time": 5894.897008895874,
  "kg_co2_emissions": null,
  "mteb_version": "1.12.85",
  "scores": {
    "test": [
      {
        "accuracy": 0.7194938440492475,
        "f1": 0.49311050927810623,
        "f1_weighted": 0.7575342358959893,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.7194938440492475,
        "scores_per_experiment": [
          {
            "accuracy": 0.7241222070223439,
            "f1": 0.480329741774798,
            "f1_weighted": 0.7612889608204841
          },
          {
            "accuracy": 0.7330141358869129,
            "f1": 0.5016115140636219,
            "f1_weighted": 0.7705213245463571
          },
          {
            "accuracy": 0.7225262197902417,
            "f1": 0.49630774323560883,
            "f1_weighted": 0.7594951115468072
          },
          {
            "accuracy": 0.7213862289101687,
            "f1": 0.4952028601113917,
            "f1_weighted": 0.7574024255312723
          },
          {
            "accuracy": 0.7346101231190151,
            "f1": 0.49787214579186284,
            "f1_weighted": 0.774966401800905
          },
          {
            "accuracy": 0.70953032375741,
            "f1": 0.4692070646155871,
            "f1_weighted": 0.749551759690971
          },
          {
            "accuracy": 0.6903784769721842,
            "f1": 0.4868464389718229,
            "f1_weighted": 0.7311878043748959
          },
          {
            "accuracy": 0.7343821249430005,
            "f1": 0.5121448249860454,
            "f1_weighted": 0.7695249815809029
          },
          {
            "accuracy": 0.7063383492932056,
            "f1": 0.5014568829895492,
            "f1_weighted": 0.7443796049591305
          },
          {
            "accuracy": 0.7186502507979936,
            "f1": 0.49012587624077414,
            "f1_weighted": 0.7570239841081677
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.7207606263982103,
        "f1": 0.4926391844090521,
        "f1_weighted": 0.7606181595994227,
        "hf_subset": "en",
        "languages": [
          "eng-Latn"
        ],
        "main_score": 0.7207606263982103,
        "scores_per_experiment": [
          {
            "accuracy": 0.7176733780760627,
            "f1": 0.4837710795251877,
            "f1_weighted": 0.7599088646563419
          },
          {
            "accuracy": 0.7306487695749441,
            "f1": 0.47305583296627834,
            "f1_weighted": 0.7691766316305646
          },
          {
            "accuracy": 0.7225950782997763,
            "f1": 0.49916514261286304,
            "f1_weighted": 0.7618078912196798
          },
          {
            "accuracy": 0.7243847874720358,
            "f1": 0.4962808940791383,
            "f1_weighted": 0.7637893008321347
          },
          {
            "accuracy": 0.731096196868009,
            "f1": 0.4871913159680292,
            "f1_weighted": 0.7704160260302501
          },
          {
            "accuracy": 0.7109619686800895,
            "f1": 0.49420419322457404,
            "f1_weighted": 0.7566142027401214
          },
          {
            "accuracy": 0.7096196868008948,
            "f1": 0.49969118032920007,
            "f1_weighted": 0.7513065484505359
          },
          {
            "accuracy": 0.7355704697986577,
            "f1": 0.5017421282678175,
            "f1_weighted": 0.7738121351976195
          },
          {
            "accuracy": 0.7033557046979866,
            "f1": 0.4766965555293341,
            "f1_weighted": 0.739019813316172
          },
          {
            "accuracy": 0.7217002237136465,
            "f1": 0.5145935215880986,
            "f1_weighted": 0.7603301819208067
          }
        ]
      }
    ]
  },
  "task_name": "MTOPIntentClassification"
}